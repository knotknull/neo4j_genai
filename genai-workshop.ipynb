{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8F3XGz_dyXc"
   },
   "source": [
    "# Neo4j Generative AI Workshop\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neo4j-product-examples/genai-workshop/blob/main/genai-workshop.ipynb)\n",
    "\n",
    "This workshop will teach you how to use Neo4j for Graph-Powered Retrieval-Augmented Generation (GraphRAG) to enhance GenAI and improve response quality for real-world applications.\n",
    "\n",
    "GenAI, despite its potential, faces challenges like hallucination and lack of domain knowledge. GraphRAG addresses these issues by combining vector search with knowledge graphs and data science techniques. This integration helps improve context, semantic understanding, and personalization, making Large Language Models (LLMs) more effective for critical applications.\n",
    "\n",
    "We walk through an example that uses real-world customer and product data from a fashion, style, and beauty retailer. We show how you can use a knowledge graph to ground an LLM, enabling it to build tailored marketing content personalized to each customer based on their interests and shared purchase histories. We use Retrieval-Augmented Generation (RAG) to accomplish this,  specifically leveraging not just vector search but also graph pattern matching and graph machine learning to provide more relevant personalized results to customers. We call this graph-powered RAG approach “GraphRAG” for short.\n",
    "\n",
    "This notebook walks through the process, including:\n",
    "- Vector search\n",
    "- Using graph patterns in Cypher to improve semantic search with context\n",
    "- Further augmenting semantic search with knowledge graph inference & ML\n",
    "- Building the LLM chain and demo app for generating content\n",
    "\n",
    "\n",
    "Please see the companion notebooks for: \n",
    "- [data-load.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/data-load.ipynb)\n",
    "    - Building the knowledge graph and generating text embeddings from scratch\n",
    "\n",
    "- [genai-example-app-only](https://github.com/neo4j-product-examples/genai-workshop/blob/main/genai-workshop-app-only.ipynb)\n",
    "    - Building the LLM chain and demo app for generating content \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmjr1dz8dyXd"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yxD7Ah0ZACB"
   },
   "source": [
    "### Some Logistics\n",
    "1. Make a copy of this notebook in Colab by [clicking here](https://colab.research.google.com/github/neo4j-product-examples/genai-workshop/blob/main/genai-workshop.ipynb).\n",
    "2. Run the pip install below to get the necessary dependencies.  this can take a while. Then run the following cell to import relevant libraries\n",
    "3. We will be using [AuraDS](https://neo4j.com/cloud/platform/aura-graph-database/)  - Neo4j's fully managed Graph Database as a service and graph data science library - to deploy this demo. \n",
    "- If you are participating in a Neo4j instructor-led training, you will be provided credentials to your own environment with pre-loaded data\n",
    "- If you are running this notebook independently, you can load a [database dump](https://storage.googleapis.com/gds-training-materials/Version8_Jan2024/neo4j_genai_hnm.dump) in your own environment or on Neo4j Desktop, or load the data using the companion notebook [data-load.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/data-load.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:14:49.948771Z",
     "start_time": "2024-06-25T14:14:46.606817Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yY1XylsiZACB",
    "outputId": "2db89b75-7918-4270-a300-8503a816ec87",
    "pycharm": {
     "name": "#%%capture\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sentence_transformers langchain langchain-openai langchain_community openai tiktoken python-dotenv gradio graphdatascience altair\n",
    "%pip install \"vegafusion[embed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:14:53.093517Z",
     "start_time": "2024-06-25T14:14:49.950586Z"
    },
    "id": "7psF1otOdyXe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import gradio as gr\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.width', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ar1ZFhPdyXe"
   },
   "source": [
    "### Setup Credentials and Environment Variables\n",
    "\n",
    "There are two things you need here.\n",
    "1. Credentials to a Neo4j database with Graph Data Science (AuraDS, Neo4j Desktop, or your own environment)\n",
    "2. Your own [OpenAI API key](https://platform.openai.com/docs/quickstart?context=python).  You can use [this one](https://docs.google.com/document/d/19Lqjd0MqRs088KUVnd23ZrVU9G0OAg-53U72VrFwwms/edit) if you do not have one already.\n",
    "\n",
    "To make this easy, you can write the credentials and env variables directly into the below cell.\n",
    "\n",
    "Alternatively, if you like, you can use an environment file. This is a best practice for the future, but fine to skip for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:14:53.098225Z",
     "start_time": "2024-06-25T14:14:53.094914Z"
    },
    "id": "BQ9s0ZWhekd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Neo4j\n",
    "NEO4J_URI = 'copy_paste_your_db_uri_here' #change this\n",
    "NEO4J_PASSWORD = 'copy_paste_your_password_here' #change this\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "AURA_DS = True\n",
    "\n",
    "# AI\n",
    "LLM = 'gpt-4o'\n",
    "os.environ['OPENAI_API_KEY'] = 'copy_paste_the_openai_key_here' #change this\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:14:53.104011Z",
     "start_time": "2024-06-25T14:14:53.100578Z"
    },
    "id": "o-98NuINdyXe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can skip this cell if not using a ws.env file - alternative to above\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if os.path.exists('ws.env'):\n",
    "     load_dotenv('ws.env', override=True)\n",
    "\n",
    "     # Neo4j\n",
    "     NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "     NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "     NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "     AURA_DS = eval(os.getenv('AURA_DS').title())\n",
    "\n",
    "     # AI\n",
    "     LLM = 'gpt-4o'\n",
    "     OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU13AU73dyXf"
   },
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "We will use the [Graph Data Science Python Client](https://neo4j.com/docs/graph-data-science-client/current/) to connect to Neo4j. This client makes it convenient to display results, as we will see later.  Perhaps more importantly, it allows us to easily run [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/introduction/) algorithms from Python.\n",
    "\n",
    "This client will only work if your Neo4j instance has Graph Data Science installed.  If not, you can still use the [Neo4j Python Driver](https://neo4j.com/docs/python-manual/current/) or use Langchain’s Neo4j Graph object that we will see later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:14:54.586538Z",
     "start_time": "2024-06-25T14:14:54.024525Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92GFeMaRdyXf",
    "outputId": "3a7a97bd-901c-40a4-ffd8-810f8a03dab9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "# Use Neo4j URI and credentials according to our setup\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=AURA_DS)\n",
    "\n",
    "# Necessary if you enabled Arrow on the db - this is true for AuraDS\n",
    "gds.set_database(\"neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO32Q0Thw5JO"
   },
   "source": [
    "Test your connection by running the below.  It should output your GDS version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:14:56.373819Z",
     "start_time": "2024-06-25T14:14:56.171335Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "QJ6qg0qMw5JO",
    "outputId": "d8c06bec-d48f-4c1b-a1a1-d6240ef2a92b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gds.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67Tm1p3LdyXe"
   },
   "source": [
    "## Knowledge Graph:\n",
    "\n",
    "<img src=\"img/hm-banner.png\" alt=\"summary\" width=\"2000\"/>\n",
    "\n",
    "This workshop will leverage the [H&M Personalized Fashion Recommendations Dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data), a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc.\n",
    "\n",
    "Below is the graph data model we will use:\n",
    "\n",
    "<img src=\"img/data-model.png\" alt=\"summary\" width=\"1000\"/>\n",
    "\n",
    "If you are interested in how the knowledge graph was created from source data, see the companion notebook [data-load.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/data-load.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:DarkBlue; background-color:LightCyan\"> \n",
    "<b>Database Tip</b>: You can view the data model in Neo4j Query Browser with the statement: </span>\n",
    "\n",
    "```cypher \n",
    "CALL db.schema.visualization()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explore the Graph\n",
    "\n",
    "Click on the node labels and relationship types on the left side of the browser window to explore the data and see what properties different node types have\n",
    "\n",
    "You can then start to build out interesting query patterns to explore the data further\n",
    "\n",
    "Some example queries you can copy paste into the browser:\n",
    "\n",
    "```cypher\n",
    "// How many Articles are in the database?\n",
    "MATCH (a:Article) RETURN COUNT(DISTINCT a) as articleCount\n",
    "\n",
    "// What are some example Articles in the database?\n",
    "MATCH (a:Article) RETURN a.prodName as product, a.productTypeName AS type  LIMIT 5\n",
    "\n",
    "// What is the most commonly purchased Article?\n",
    "MATCH (c:Customer)-[r:PURCHASED]->(a:Article)\n",
    "RETURN a.prodName as product, count(r) as purchases ORDER BY purchases DESC LIMIT 5                                \n",
    "\n",
    "// What Department has the most purchases?\n",
    "MATCH (c:Customer)-[r:PURCHASED]->(:Article)-[:FROM_DEPARTMENT]->(d:Department)\n",
    "RETURN d.departmentName as department, count(r) as purchases ORDER BY purchases DESC LIMIT 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpKcGgRZdyXg"
   },
   "source": [
    "## Vector Search\n",
    "\n",
    "In this section, we will use text embeddings that were previously generated from product descriptions and demonstrate how to leverage the Neo4j vector index for vector search. We will also introduce the use of [LangChain](https://www.langchain.com/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGoMu3bqekeE"
   },
   "source": [
    "### Vector Search Using Cypher\n",
    "Embeddings enable us to reduce multidimensional information into an embedded vector. In this case, we are using a semantic text embedding to support search for semantically similar products. This vector search provides advantages over traditional keyword search techniques as it defines similarity based on the meaning of the search terms and the returned items, rather than text similarity. \n",
    "\n",
    "To prepare a vector search over products, we need to:\n",
    "1. Convert product text descriptions into embeddings\n",
    "2. Store these semantic embeddings as node properties in the database \n",
    "3. Create a vector index to enable rapid search\n",
    "\n",
    "These first steps were completed for you and are covered in [data-load.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/data-load.ipynb)\n",
    "\n",
    "To perform vector search, we need to:\n",
    "1. Take the search prompt and convert it to an embedding query vector\n",
    "2. Use similarity search with that new vector to pull semantically similar documents\n",
    "\n",
    "\n",
    "The [Neo4j Vector Index](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/) enables efficient Approximate Nearest Neighbor (ANN) search with vectors. It uses the Hierarchical Navigable Small World (HNSW) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN7zU0GbZACF"
   },
   "source": [
    "\n",
    "\n",
    "Below is an example of converting a search prompt into a query vector. Again we can use Neo4j’s native integrations to call embedding APIs from Cypher. We use our same embedding model, OpenAI `text-embedding-ada-002` by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:17:00.619881Z",
     "start_time": "2024-06-25T14:16:59.683818Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "JrywpfKFdzh4",
    "outputId": "8da80838-b6f3-4826-a9a5-64c424921900",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search_prompt = 'denim jeans, loose fit, high-waist'\n",
    "search_prompt = 'denim jeans'\n",
    "#generate embeddings\n",
    "gds.run_cypher('''\n",
    "RETURN genai.vector.encode($searchPrompt, \"OpenAI\", {token:$token}) AS queryVector\n",
    "''', params={'searchPrompt':search_prompt, 'token':OPENAI_API_KEY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDTQ3iE0dzh4"
   },
   "source": [
    "To conduct vector search, we simply combine the above query with a call to the vector index to retrieve semantically similar documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:17:13.388284Z",
     "start_time": "2024-06-25T14:17:12.782844Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "T2nck70Wdzh4",
    "outputId": "a9e1f339-b218-452c-f396-a482e02b1074",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate embeddings\n",
    "vector_res = gds.run_cypher('''\n",
    "\n",
    "//convert search prompt to query vector\n",
    "WITH genai.vector.encode(\n",
    "  $searchPrompt,\n",
    "  \"OpenAI\",\n",
    "  {token:$token}) AS queryVector\n",
    "\n",
    "//find similar products via vector search with index\n",
    "CALL db.index.vector.queryNodes(\"product_text_embeddings\", 10, queryVector)\n",
    "YIELD node AS product, score\n",
    "RETURN product.productCode AS productCode,\n",
    "    product.text AS text,\n",
    "    score\n",
    "''', params={'searchPrompt':search_prompt, 'token':OPENAI_API_KEY})\n",
    "\n",
    "vector_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4UlyNKcdzh5"
   },
   "source": [
    "While the above Cypher vector encoding calls are convenient, you have the freedom to generate embeddings externally from any model/API you want. You can then ingest them into Neo4j and use them for vector search.\n",
    "Below is an example of generating an embedding externally and sending it as a query parameter, in this case for search, but you can also load embeddings in a similar matter.\n",
    "\n",
    "**It is important to use the same embedding model for your search term that you used when embedding the data in your search space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:17:23.380365Z",
     "start_time": "2024-06-25T14:17:23.374640Z"
    },
    "id": "elNIp0_BdyXo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Optional for workshop\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "#\n",
    "# embedding_model = OpenAIEmbeddings()\n",
    "# embedding_dimension = 1536\n",
    "#\n",
    "# query_vector = embedding_model.embed_query(search_prompt)\n",
    "# print(f'query vector length: {len(query_vector)}')\n",
    "# print(f'query vector sample: {query_vector[:10]}')\n",
    "# print(f'Result:')\n",
    "#\n",
    "# gds.run_cypher('''\n",
    "# CALL db.index.vector.queryNodes(\"product_text_embeddings\", 10, $queryVector)\n",
    "# YIELD node AS product, score\n",
    "# RETURN product.productCode AS productCode,\n",
    "#     product.text AS text,\n",
    "#     score\n",
    "# ''', params={'queryVector': query_vector})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualizing Results\n",
    "\n",
    "Sometimes you may want to explore results outside of a table view. We can also visualize these search results in the graph. In this case - we will just see the product nodes returned\n",
    "\n",
    "<img src=\"img/search_results.png\" alt=\"summary\" width=\"500\"/>\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\"> \n",
    "<b>Database Tip</b>: You can view your results by copying the query printed below into Neo4j</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"MATCH (p:Product)\\nWHERE p.productCode IN {productList}\\nRETURN p\".format(productList= list(vector_res.productCode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLmyZ_5lhTRm"
   },
   "source": [
    "### Vector Search Using Langchain\n",
    "\n",
    "We can also do this vector search with [LangChain](https://www.langchain.com/), a recommended approach as we seek to integrate LLM chains going forward.  To do this, we use the `Neo4jVector` class and call the below method to set up from an existing index in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:17:30.518564Z",
     "start_time": "2024-06-25T14:17:29.231314Z"
    },
    "id": "ofoi5aJvekeF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "embedding_dimension = 1536\n",
    "\n",
    "kg_vector_search = Neo4jVector.from_existing_index(\n",
    "    embedding=embedding_model,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name='product_text_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz1VqGy9ZACG"
   },
   "source": [
    "Langchain can handle embedding the query vector and retrieving data from Neo4j behind the scenes, making our lives easier.  Langchain uses a similar query as above and retrieves the `text` property we set for each Product node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:17:32.017369Z",
     "start_time": "2024-06-25T14:17:31.660776Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDFoMXmbekeF",
    "outputId": "9714fc4b-17b1-48e1-ab74-a6f4123c5a88",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instead of 2 steps (Embed search term, perform vector search) \n",
    "# LangChain handles the orchestration for us in a single line of code\n",
    "res = kg_vector_search.similarity_search(search_prompt, k=10)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:17:38.268022Z",
     "start_time": "2024-06-25T14:17:38.263749Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "qQP7e2dcekeF",
    "outputId": "97d03a9b-a7dc-42cd-ea77-13c1aad242ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize as a dataframe\n",
    "pd.DataFrame([{'document': d.page_content} for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXn_6bFCekeF"
   },
   "source": [
    "### Try Yourself\n",
    "\n",
    "Experiment with your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:17:43.270703Z",
     "start_time": "2024-06-25T14:17:42.964736Z"
    },
    "id": "rHayKAOlekeF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = kg_vector_search.similarity_search('type your prompt here!', k=10)\n",
    "pd.DataFrame([{'document': d.page_content} for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tofjZIsSekeF"
   },
   "source": [
    "## Semantic Search with Context (Graph Patterns)\n",
    "__Using Graph Patterns to Improve Context in Search & Retrieval__\n",
    "\n",
    "Above, we saw how you can use the vector index to find semantically similar products in user searches.  This is a powerful tool; however, it can be improved upon.  We have more data in this dataset that could be helpful in making recommendations - like customer purchase history.\n",
    "\n",
    "Furthermore, some search\n",
    "prompts, like \"denim jeans\", are very general and can match a large number of products, many of which won't be relevant to the specific user conducting the search.\n",
    "\n",
    "We have a rich knowledge graph full of customer information; let's see how to leverage it to improve search experience and create more personalized search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NrRQaY3ZACH"
   },
   "source": [
    "### Explore in Neo4j\n",
    "To understand how to better leverage the data in our graph, let's explore in Neo4j Browser/Query view on our Aura instance.\n",
    "\n",
    "We can also use Cypher to sample the graph.\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\"> \n",
    "<b>Database Tip</b>: Run the below query in Browser and explore the results </span>\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Product)<-[v:VARIANT_OF]-(a:Article)<-[t:PURCHASED]-(c:Customer)\n",
    "RETURN * LIMIT 150\n",
    "```\n",
    "\n",
    "You can combine this query pattern with the products that resulted from our vector search in the printed cypher query in the next cell\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\"> \n",
    "<b>Database Tip</b>: See the connections between vector search results with the printed query result below </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"MATCH (p:Product)<-[v:VARIANT_OF]-(a:Article)<-[t:PURCHASED]-(c:Customer) \\nWHERE p.productCode IN {productList}\\nRETURN *\".format(productList= list(vector_res.productCode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NrRQaY3ZACH"
   },
   "source": [
    "### Graph Connections! \n",
    "You should get something that looks like the below.  Notice the multi-hop connections between customers based on purchases. This is valuable information encoded in our graph!\n",
    "\n",
    "<img src=\"img/sample-query.png\" alt=\"summary\" width=\"1000\"/>\n",
    "\n",
    "#### Understanding Shared Customer Behavior\n",
    "\n",
    "Now let's consider a single customer's purchase history.\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\">\n",
    "<b>Database Tip</b>: We will choose the below customer by setting customerId as a parameter. </span>\n",
    "\n",
    "```cypher\n",
    ":params {customerId:'daae10780ecd14990ea190a1e9917da33fe96cd8cfa5e80b67b4600171aa77e0'}\n",
    "```\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\">\n",
    "<b>Database Tip</b>: Then we can run the below Cypher to pull our target customers's purchase history </span>\n",
    "\n",
    "\n",
    "```cypher\n",
    "MATCH(c:Customer {customerId: $customerId})-[t:PURCHASED]->(:Article)\n",
    "-[:VARIANT_OF]->(p:Product)\n",
    "RETURN p.productCode AS productCode,\n",
    "    p.prodName AS prodName,\n",
    "    p.productTypeName AS productTypeName,\n",
    "    p.garmentGroupName AS garmentGroupName,\n",
    "    p.detailDesc AS detailDesc,\n",
    "    t.tDat AS purchaseDate\n",
    "ORDER BY t.tDat DESC\n",
    "```\n",
    "Expected results:\n",
    "<img src=\"img/purchase-history.png\" alt=\"summary\" width=\"1000\"/>\n",
    "\n",
    "These purchases are ordered by transaction date. The most recent purchases should be the \"Tove Top\" and the \"Rosemary Dress\".\n",
    "\n",
    "Now let's consider just the latest products in the above list and see what else we could recommend to customers who liked them.\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\">\n",
    "<b>Database Tip</b>: The following Cypher query provides potential answers by finding the most popular products among customers who purchased these. </span>\n",
    "\n",
    "```cypher\n",
    "//pull the latest purchases\n",
    "MATCH(c:Customer {customerId: $customerId})-[t:PURCHASED]->()\n",
    "WITH max(t.tDat) AS latestPurchases\n",
    "//find related products based on customer purchases\n",
    "MATCH(c:Customer {customerId: $customerId})-[:PURCHASED {tDat: latestPurchases}]->(:Article)<-[:PURCHASED]-(:Customer)-[:PURCHASED]->(:Article)\n",
    "    -[:VARIANT_OF]->(p:Product)\n",
    "RETURN p.productCode AS productCode,\n",
    "    p.prodName AS prodName,\n",
    "    p.productTypeName AS productTypeName,\n",
    "    p.garmentGroupName AS garmentGroupName,\n",
    "    count(*) AS commonPurchaseScore,\n",
    "    p.detailDesc AS detailDesc\n",
    "ORDER BY commonPurchaseScore DESC\n",
    "```\n",
    "\n",
    "Expected results:\n",
    "<img src=\"img/related-products.png\" alt=\"summary\" width=\"1000\"/>\n",
    "\n",
    "__You will see that some of the above results seem intuitive, like we see other products of the type \"Jersey Fancy\" which our customer has previously purchased...but not all of them...and that is exactly the point!\n",
    "There is information encoded inside the knowledge graph about customer preferences that isn't inferable from the product text documents.__\n",
    "\n",
    "__This is one example of where enterprise-specific data, expressed as structured relationships, contains critical information that is impossible to find elsewhere. This is why, for many real-world applications, you should consider backing semantic search and GenAI with Knowledge Graphs.__\n",
    "\n",
    "Now let’s see how to apply this pattern in our semantic search and retrieval!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdiPqIq2dyXp"
   },
   "source": [
    "### Personalizing Results Based on Customer Behavior in the Graph\n",
    "\n",
    "As we saw in Browser, an important piece of information expressed in this graph, but not directly in the product documents and text embeddings, is customer purchasing behavior.  We saw that we can use graph patterns in Cypher to extract insights from these. Now that we know how this pattern works, we can apply it to our semantic search to make results more personalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0ySzeloZACI"
   },
   "source": [
    "To do this, we append a MATCH statement to the end of our initial vector search query.  Basically, once the product documents are returned, we can re-calculate how they would score according to the query above and use that to re-rank the search results.\n",
    "\n",
    "Langchain makes this easy by allowing for a `retrieval_query` argument where we can put in the pattern we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:22:05.871433Z",
     "start_time": "2024-06-25T14:22:05.084292Z"
    },
    "id": "8eKwXKf_ekeF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "kg_personalized_search = Neo4jVector.from_existing_index(\n",
    "    embedding=embedding_model,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name='product_text_embeddings',\n",
    "    retrieval_query=\"\"\"\n",
    "    WITH node AS product, score AS searchScore\n",
    "\n",
    "    OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n",
    "    -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {customerId: $customerId})\n",
    "\n",
    "    WITH count(a) AS purchaseScore, product.text AS text, searchScore, product.productCode AS productCode\n",
    "    RETURN text,\n",
    "        (1+purchaseScore)*searchScore AS score,\n",
    "        {productCode: productCode, purchaseScore:purchaseScore, searchScore:searchScore} AS metadata\n",
    "    ORDER BY purchaseScore DESC, searchScore DESC LIMIT 15\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BqGWliZZACI"
   },
   "source": [
    "The query expands from the results of the pure vector search to prioritize products that similar customers purchased, with the new combined __score__ reflecting both the text embedding similarity and number of purchases in common with our target customer\n",
    "\n",
    "🎉 This allows us to rank results by BOTH semantic similarity and customer behavior! 🎉\n",
    "\n",
    "Now let's run it to see if/how our results have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:22:08.039320Z",
     "start_time": "2024-06-25T14:22:07.129705Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "rUCSQyrZekeF",
    "outputId": "92b255ea-2371-4cc4-8248-eea9587b78d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUSTOMER_ID = \"daae10780ecd14990ea190a1e9917da33fe96cd8cfa5e80b67b4600171aa77e0\"\n",
    "\n",
    "res = kg_personalized_search.similarity_search(search_prompt, k=100, params={'customerId': CUSTOMER_ID})\n",
    "\n",
    "# Visualize as a dataframe\n",
    "vector_kg_res = pd.DataFrame([{'productCode': d.metadata['productCode'],\n",
    "               'document': d.page_content,\n",
    "               'searchScore': d.metadata['searchScore'],\n",
    "               'purchaseScore': d.metadata['purchaseScore']} for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare our results, we'll merge our `vector-only` results dataframe and our `vector+knowledge graph traversal` results dataframe on product code and capture the overlapping product row indexes in a new column \"vector_rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_res.reset_index(names='vector_rank', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - the results have changed quite a bit! These results are specifically tailored to our target customer based on their preferences, rather than just being semantically related to our search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_kg_res.merge(vector_res[['productCode', 'vector_rank']], on='productCode', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t55maEsfdyXp"
   },
   "source": [
    "## Augmenting Semantic Search with Knowledge Graph Inference & ML\n",
    "\n",
    "We saw above how to use graph pattern matching to personalize semantic search and make it more contextually relevant.\n",
    "\n",
    "In addition to this, we also have [Graph Data Science algorithms and machine learning](https://neo4j.com/docs/graph-data-science/current/introduction/) which allows you to enrich your knowledge graph with additional properties, relationships, and graph metrics. These can, in turn, be leveraged in search and retrieval to improve and augment results.\n",
    "\n",
    "We will walk through an example of this below, where we use Graph Data Science to augment retrieval with additional product recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vec2fJG7ZACI"
   },
   "source": [
    "Below, we use graph machine learning to create relationships that can help us make personalized recommendations based on purchase history.\n",
    "\n",
    "\n",
    "We do this by leveraging Node Embeddings with K-Nearest Neighbor (KNN). Specifically we:\n",
    "1. Use Node embeddings to encode the similarity between articles based on shared customer search history\n",
    "2. Take those node embeddings as input to KNN, an unsupervised learning technique, to link the most similar products together with a \"CUSTOMERS_ALSO_LIKE\" relationship.\n",
    "3. We can then use Cypher patterns at query time to grab recommended items based on a customer's recent purchase history.\n",
    "This process helps scale memory-based recommendation techniques to larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:57:31.454259Z",
     "start_time": "2024-06-25T14:57:23.945870Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624,
     "referenced_widgets": [
      "d6fafe3e88794416a0a648d49cf99a2f",
      "3f61044daef546188f0fe35f3eac0753",
      "db73dfccd8df444ca1a899ce61a897a8",
      "28268c992fb14884a4b6d6baf7f38d90",
      "58aa4246b12f49beb51301184d84631c",
      "09f9102594964a5ab35a74835df69449",
      "e448ee34e08a4b269dba969360d7b669",
      "f5f1092f2b754e1c952f088e7d2058a2",
      "16ffa81fccf4427d9e51eac6105202a2",
      "72129337625d4b9c9d534c79fdb8bc8e",
      "4776c602b8e04eefa4057d1da7958ef3"
     ]
    },
    "id": "PtGT6HfgekeG",
    "outputId": "836d670a-cd11-4095-a182-3a0bb8349fb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#clear past GDS analysis in the case of re-running\n",
    "def clear_all_graphs():\n",
    "    g_names = gds.graph.list().graphName.tolist()\n",
    "    for g_name in g_names:\n",
    "        g = gds.graph.get(g_name)\n",
    "        g.drop()\n",
    "\n",
    "clear_all_graphs()\n",
    "\n",
    "gds.run_cypher('''\n",
    "    MATCH(:Article)-[r:CUSTOMERS_ALSO_LIKE]->()\n",
    "    CALL {\n",
    "        WITH r\n",
    "        DELETE r\n",
    "    } IN TRANSACTIONS OF 1000 ROWS\n",
    "    ''')\n",
    "\n",
    "# graph projection - project co-purchase graph into analytics workspace\n",
    "gds.run_cypher('''\n",
    "   MATCH (a1:Article)<-[:PURCHASED]-(:Customer)-[:PURCHASED]->(a2:Article)\n",
    "   WITH gds.graph.project(\"proj\", a1, a2,\n",
    "       {sourceNodeLabels: labels(a1),\n",
    "       targetNodeLabels: labels(a2),\n",
    "       relationshipType: \"COPURCHASE\"}) AS g\n",
    "   RETURN g.graphName\n",
    "   ''')\n",
    "# This results in an in-memory graph with the structure (Article)-[:COPURCHASE]->(Article)\n",
    "\n",
    "g = gds.graph.get(\"proj\")\n",
    "\n",
    "# create FastRP node embeddings\n",
    "gds.fastRP.mutate(g, mutateProperty='nodeEmbedding', embeddingDimension=128, randomSeed=7474, concurrency=4, iterationWeights=[0.0, 1.0, 1.0])\n",
    "\n",
    "# write embeddings back to database to introspect later\n",
    "gds.graph.writeNodeProperties(g, ['nodeEmbedding'], ['Article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Node Embeddings\n",
    "We applied [FastRP](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp/) to capture the topological structure of customer copurchase behavior. These embeddings will enable us to evaluate similarity between node embeddings to understand whether product articles are likely to be purchased together, and persist relationships we can use to enhance our recommendations\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\">\n",
    "<b>Database Tip</b>: Use the following query to view node embedding properties in the database </span>\n",
    "\n",
    "``` cypher\n",
    "MATCH (p:Product)<-[:VARIANT_OF]-(a:Article)-[:FROM_DEPARTMENT]-(d)\n",
    "RETURN a.articleId AS articleId,\n",
    "    p.prodName AS productName,\n",
    "    p.productTypeName AS productTypeName,\n",
    "    d.departmentName AS departmentName,\n",
    "    d.sectionName AS sectionName,\n",
    "    p.detailDesc AS detailDesc,\n",
    "    a.nodeEmbedding AS nodeEmbedding LIMIT 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) Relationships\n",
    "\n",
    "Now, we can do our similarity inference with [K-Nearest Neighbors algorithm (KNN)](https://neo4j.com/docs/graph-data-science/current/algorithms/knn/) - writing results back directly to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:57:31.454259Z",
     "start_time": "2024-06-25T14:57:23.945870Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624,
     "referenced_widgets": [
      "d6fafe3e88794416a0a648d49cf99a2f",
      "3f61044daef546188f0fe35f3eac0753",
      "db73dfccd8df444ca1a899ce61a897a8",
      "28268c992fb14884a4b6d6baf7f38d90",
      "58aa4246b12f49beb51301184d84631c",
      "09f9102594964a5ab35a74835df69449",
      "e448ee34e08a4b269dba969360d7b669",
      "f5f1092f2b754e1c952f088e7d2058a2",
      "16ffa81fccf4427d9e51eac6105202a2",
      "72129337625d4b9c9d534c79fdb8bc8e",
      "4776c602b8e04eefa4057d1da7958ef3"
     ]
    },
    "id": "PtGT6HfgekeG",
    "outputId": "836d670a-cd11-4095-a182-3a0bb8349fb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw KNN\n",
    "knn_stats = gds.knn.write(g, nodeProperties=['nodeEmbedding'], nodeLabels=['Article'],\n",
    "                  writeRelationshipType='CUSTOMERS_ALSO_LIKE', writeProperty='score',\n",
    "                  sampleRate=1.0, initialSampler='randomWalk', concurrency=1, similarityCutoff=0.75, randomSeed=7474)\n",
    "\n",
    "# clear graph projection once done\n",
    "g.drop()\n",
    "\n",
    "# output knn stats\n",
    "knn_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new persisted relationship capturing the similarity between articles based on customer preferences inferred from their product\n",
    "copurchase behavior.\n",
    "\n",
    "<span style=\"color:DarkBlue; background-color:LightCyan\">\n",
    "<b>Database Tip</b>: Refresh your browser window and click on the new relationship CUSTOMERS_ALSO_LIKE on the left side panel to see the inferred relationship from FastRP and KNN </span>\n",
    "\n",
    "<img src=\"img/CUSTOMERS_ALSO_LIKE.png\" alt=\"summary\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nNobdJhekeH"
   },
   "source": [
    "### Personalized Recommendations\n",
    "\n",
    "Now, let's construct a KG store to retrieve recommendations for a user using our new CUSTOMERS_ALSO_LIKE relationship.  This need not be based on a user prompt or vector search. Instead, we will make it purely based on purchase history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:57:44.104952Z",
     "start_time": "2024-06-25T14:57:43.136597Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "NVy9J6T5ekeH",
    "outputId": "1e46c500-4309-448f-90d7-6ad7b13e5926",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "kg = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)\n",
    "\n",
    "res = kg.query('''\n",
    "    MATCH(:Customer {customerId:$customerId})-[:PURCHASED]->(:Article)\n",
    "    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n",
    "    RETURN product.productCode AS productCode,\n",
    "        product.prodName AS prodName,\n",
    "        product.productTypeName AS productType,\n",
    "        product.text AS document,\n",
    "        sum(r.score) AS recommenderScore\n",
    "    ORDER BY recommenderScore DESC LIMIT $k\n",
    "    ''', params={'customerId': CUSTOMER_ID, 'k':15})\n",
    "\n",
    "#visualize as dataframe. result is list of dict\n",
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90E9HGu4dyXq"
   },
   "source": [
    "## LLM For Generating Grounded Content\n",
    "\n",
    "Let's use an LLM to automatically generate content for targeted marketing campaigns grounded with our knowledge graph using the above tools.\n",
    "Here is a quick example for generating promotional messages, but you can create all sorts of content with this!\n",
    "\n",
    "For our first message, let's consider a scenario where a user recently searched for products, but perhaps didn't commit to a purchase yet. We now want to send a message to promote relevant products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:12:37.528150Z",
     "start_time": "2024-06-25T15:12:37.496780Z"
    },
    "id": "JI9LVEdKekeH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "#Instantiate LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=LLM, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I6JesV0ekeH"
   },
   "source": [
    "### Create Knowledge Graph Stores for Retrieval\n",
    "\n",
    "To ground our content generation, we need to define retrievers to pull information from our knowledge graph.  Let's make two stores:\n",
    "1. Personalized Search Retriever (`kg_personalized_search`): Based on recent customer searches and purchase history, pull relevant products.\n",
    "2. Recommendations retriever (`kg_recommendations_app`): Based on our Graph ML, what else can we recommend to them that pairs with their interests?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:12:39.955168Z",
     "start_time": "2024-06-25T15:12:39.307879Z"
    },
    "id": "WLBBVRXwdyXq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will use a mock URL for our sources in the metadata\n",
    "kg_personalized_search_store = Neo4jVector.from_existing_index(\n",
    "        embedding=embedding_model,\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD,\n",
    "        index_name='product_text_embeddings',\n",
    "        retrieval_query=\"\"\"\n",
    "        WITH node AS product, score AS searchScore\n",
    "\n",
    "        OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n",
    "        -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {customerId: $customerId})\n",
    "        WITH count(a) AS purchaseScore, product, searchScore\n",
    "        RETURN product.text + '\\nurl: ' + product.url  AS text,\n",
    "            (1.0+purchaseScore)*searchScore AS score,\n",
    "            {source: product.url} AS metadata\n",
    "        ORDER BY purchaseScore DESC, searchScore DESC LIMIT 10\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# This will be a function so we can change per customer id\n",
    "def kg_personalized_search(search_prompt, customer_id, k=100):\n",
    "    docs = kg_personalized_search_store.similarity_search(search_prompt, k, params={'customerId': customer_id})\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Use the same personalized recommendations as above but with a smaller limit\n",
    "def kg_recommendations_app(customer_id, k=30):\n",
    "    res = kg.query(\"\"\"\n",
    "    MATCH(:Customer {customerId:$customerId})-[:PURCHASED]->(:Article)\n",
    "    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n",
    "    RETURN product.text + '\\nurl: ' + product.url   AS text,\n",
    "        sum(r.score) AS recommenderScore\n",
    "    ORDER BY recommenderScore DESC LIMIT $k\n",
    "    \"\"\", params={'customerId': customer_id, 'k':k})\n",
    "\n",
    "    return \"\\n\\n\".join([d['text'] for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7sCt8roekeH"
   },
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "Now let's define our prompt. We will accept multiple parameters and provide detailed instructions to the LLM to condition the response based of retrieved data, customer interests, and time of year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:12:41.328780Z",
     "start_time": "2024-06-25T15:12:41.326620Z"
    },
    "id": "aUAROR6aekeI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template('You are a personal assistant named Sally '\n",
    "'for a fashion, home, and beauty company called HRM.'\n",
    "'write an engaging email to {customerName}, one of your customers, '\n",
    "'to promote and summarize products relevant for them given: '\n",
    "'- The current season / time of year: {timeOfYear}'\n",
    "'- Recent searches/interests: {customerInterests}'\n",
    "'Please only mention the products listed below. '\n",
    "'Do not come up with or add any new products to the list.'\n",
    "'Each product comes with an https `url` field. '\n",
    "'Make sure to provide that https url with descriptive name text '\n",
    "'in markdown for each product.'\n",
    "'''\n",
    "\n",
    "# RelevantProducts:\n",
    "These are products from the HRM store the customer may be interested in based\n",
    "on their recent searches/interests: {customerInterests}\n",
    "{searchProds}\n",
    "\n",
    "# Customer May Also Be Interested In the following\n",
    "The below candidates are recommended based on the shared purchase patterns of\n",
    "other customers in the HRM database.\n",
    "Select the best 4 to 5 product subset from the context that best match the\n",
    "time of year: {timeOfYear} and to pair with the RelevantProducts above.\n",
    "For example, even if scarfs are listed here, they may not be appropriate for a\n",
    "summer time of year so best not to include those.\n",
    "{recProds}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgFbaUt6ekeI"
   },
   "source": [
    "### Create a Chain\n",
    "\n",
    "Now let's put a chain together that will leverage the retrievers, prompt, and LLM model. This is where Langchain shines, putting RAG together in a simple way.\n",
    "\n",
    "In addition to the personalized search and recommendations context, we will allow for some other parameters.\n",
    "\n",
    "1. `timeOfYear`: The time of year as a date, season, month, etc. so the LLM can tailor language and choose relevant products appropriately.\n",
    "2. `customerName`: Ordinarily, this can be pulled from the DB, but it has been scrubbed to maintain anonymity, so we will provide our own name here.\n",
    "\n",
    "You can potentially add other creative parameters here to help the LLM write relevant messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:12:43.746533Z",
     "start_time": "2024-06-25T15:12:43.742013Z"
    },
    "id": "nUpih07QdyXr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = ({'searchProds': (lambda x:kg_personalized_search(x['customerInterests'], x['customerId'])),\n",
    "          'recProds': (lambda x:kg_recommendations_app(x['customerId'])),\n",
    "          'customerName': lambda x:x['customerName'],\n",
    "          'timeOfYear': lambda x:x['timeOfYear'],\n",
    "          \"customerInterests\":  lambda x:x['customerInterests']}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjbUGH6WekeI"
   },
   "source": [
    "### Example Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:13:00.764442Z",
     "start_time": "2024-06-25T15:12:46.133584Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jL6P3IoydyXr",
    "outputId": "a852420f-0011-46bc-c8e2-21430cf30d6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({'customerInterests':search_prompt,\n",
    "                    'customerId':CUSTOMER_ID,\n",
    "                    'customerName':'Alex Smith',\n",
    "                    'timeOfYear':'Jun, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy3fKoC1E0CC"
   },
   "source": [
    "#### Inspecting the Prompt Sent to the LLM\n",
    "In the above run, the LLM should only be using results from our Neo4j database to populate recommendations. Run the below cell to see the final prompt that was sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:13:01.222535Z",
     "start_time": "2024-06-25T15:13:00.766490Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7-yDDUaD6FD",
    "outputId": "a1b962f7-07fc-4d57-efe4-64ba5429b1dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_final_prompt(x):\n",
    "   return f'''=== Prompt to send to LLM ===\n",
    "   {x.to_string()}\n",
    "   === End Prompt ===\n",
    "   '''\n",
    "chain_print_prompt = ({'searchProds': (lambda x:kg_personalized_search(x['customerInterests'], x['customerId'])),\n",
    "                            'recProds': (lambda x:kg_recommendations_app(x['customerId'])),\n",
    "                            'customerName': lambda x:x['customerName'],\n",
    "                            'timeOfYear': lambda x:x['timeOfYear'],\n",
    "                            \"customerInterests\":  lambda x:x['customerInterests']}\n",
    "         | prompt\n",
    "         | format_final_prompt\n",
    "         | StrOutputParser())\n",
    "\n",
    "print( chain_print_prompt.invoke({\n",
    "    'customerInterests':search_prompt,\n",
    "    'customerId':CUSTOMER_ID,\n",
    "    'customerName':'Alex Smith',\n",
    "    'timeOfYear':'Feb, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G_vdFviekeI"
   },
   "source": [
    "Feel free to experiment and try more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:13:31.279029Z",
     "start_time": "2024-06-25T15:13:01.224882Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeOts3Q4ZACL",
    "outputId": "df5e0270-bc27-47d0-883a-26b5af1ac72b"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({'customerInterests':\"western boots\",\n",
    "                    'customerId':CUSTOMER_ID,\n",
    "                    'customerName':'Alex Smith',\n",
    "                    'timeOfYear':'July, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IU_gedrekeI"
   },
   "source": [
    "### Demo App\n",
    "Now let’s use the above tools to create a demo app with Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:13:31.286051Z",
     "start_time": "2024-06-25T15:13:31.281781Z"
    },
    "id": "A1F0ve3cekeI"
   },
   "outputs": [],
   "source": [
    "# create multiple demo examples to try\n",
    "examples = [\n",
    "    [\n",
    "        CUSTOMER_ID,\n",
    "        'June, 2024',\n",
    "        'Alex Smith',\n",
    "        'denim jeans'\n",
    "    ],\n",
    "    [\n",
    "        CUSTOMER_ID,\n",
    "        'July, 2024',\n",
    "        'Alex Smith',\n",
    "        'western boots'\n",
    "    ],\n",
    "    [\n",
    "        '819f4eab1fd76b932fd403ae9f427de8eb9c5b64411d763bb26b5c8c3c30f16f',\n",
    "        'June, 2024',\n",
    "        'Robin Fischer',\n",
    "        'denim jeans'\n",
    "    ],\n",
    "    [\n",
    "        '44b0898ecce6cc1268dfdb0f91e053db014b973f67e34ed8ae28211410910693',\n",
    "        'Feb, 2024',\n",
    "        'Chris Johnson',\n",
    "        'Oversized Sweaters'\n",
    "    ],\n",
    "    [\n",
    "        '819f4eab1fd76b932fd403ae9f427de8eb9c5b64411d763bb26b5c8c3c30f16f',\n",
    "        'Feb, 2024',\n",
    "        'Robin Fischer',\n",
    "        'denim jeans'\n",
    "    ],\n",
    "    [\n",
    "        CUSTOMER_ID,\n",
    "        'Feb, 2024',\n",
    "        'Alex Smith',\n",
    "        'oversized sweaters'\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:13:54.112111Z",
     "start_time": "2024-06-25T15:13:31.287796Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "XsBcFQLlekeI",
    "outputId": "9ac8faf5-44ed-45c7-9f2f-62ea9c3101c8"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def message_generator(*x):\n",
    "    return chain.invoke({'customerInterests':x[3],\n",
    "                         'customerId':x[0],\n",
    "                         'customerName':x[2],\n",
    "                         'timeOfYear': x[1]})\n",
    "\n",
    "customer_id = gr.Textbox(value=CUSTOMER_ID, label=\"Customer ID\")\n",
    "time_of_year = gr.Textbox(value=\"June, 2024\", label=\"Time Of Year\")\n",
    "search_prompt_txt = gr.Textbox(value='denim jeans', label=\"Customer Interests(s)\")\n",
    "customer_name = gr.Textbox(value='Alex Smith', label=\"Customer Name\")\n",
    "message_result = gr.Markdown( label=\"Message\")\n",
    "\n",
    "demo = gr.Interface(fn=message_generator,\n",
    "                    inputs=[customer_id, time_of_year, customer_name, search_prompt_txt],\n",
    "                    outputs=message_result,\n",
    "                    examples=examples,\n",
    "                    title=\"🪄 Message Generator 🥳\")\n",
    "\n",
    "if not os.getenv('AUTOMATED_RUN') == \"true\":\n",
    "    demo.launch(share=True, debug=True) # NOTE - change share=False if you are running locally to use localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tucjvc73ZACM"
   },
   "source": [
    "## That's a Wrap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09f9102594964a5ab35a74835df69449": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16ffa81fccf4427d9e51eac6105202a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "28268c992fb14884a4b6d6baf7f38d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72129337625d4b9c9d534c79fdb8bc8e",
      "placeholder": "​",
      "style": "IPY_MODEL_4776c602b8e04eefa4057d1da7958ef3",
      "value": " 100.0/100 [00:05&lt;00:00, 32.08%/s]"
     }
    },
    "3f61044daef546188f0fe35f3eac0753": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09f9102594964a5ab35a74835df69449",
      "placeholder": "​",
      "style": "IPY_MODEL_e448ee34e08a4b269dba969360d7b669",
      "value": "Knn: 100%"
     }
    },
    "4776c602b8e04eefa4057d1da7958ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58aa4246b12f49beb51301184d84631c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72129337625d4b9c9d534c79fdb8bc8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6fafe3e88794416a0a648d49cf99a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f61044daef546188f0fe35f3eac0753",
       "IPY_MODEL_db73dfccd8df444ca1a899ce61a897a8",
       "IPY_MODEL_28268c992fb14884a4b6d6baf7f38d90"
      ],
      "layout": "IPY_MODEL_58aa4246b12f49beb51301184d84631c"
     }
    },
    "db73dfccd8df444ca1a899ce61a897a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5f1092f2b754e1c952f088e7d2058a2",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16ffa81fccf4427d9e51eac6105202a2",
      "value": 100
     }
    },
    "e448ee34e08a4b269dba969360d7b669": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5f1092f2b754e1c952f088e7d2058a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
